<!DOCTYPE html><html lang="en" dir="ltr"><head><link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,300..800;1,300..800&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="X-UA-Compatible" content="ie=edge"><title> | Chandradeep Pokhariya</title><meta name="description" content=""><link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/favicon.ico" type="image/x-icon"><link href='/feed.xml' rel='alternate' type='application/atom+xml'><link rel="canonical" href="/diff_param"><link rel="stylesheet" href="/assets/style.css"><body><main> <center><h1> Discretization-Agnostic Deep Self-Supervised 3D Surface Parameterization</h1>(Siggraph-Asia Technical Communications 2022) </center><hr /><div class="is-size-6 publication-authors" align="center"> <span class="author-block"> <a href="https://coreqode.github.io">Chandradeep Pokhariya*</a><sup>1</sup>,</span> <span class="author-block"> <a href="https://shanthika.github.io">Shanthika Naik*</a><sup>1</sup>,</span> <span class="author-block"> <a href="http://astitvasri.github.io">Astitva Srivastava</a><sup>1</sup>, </span> <span class="author-block"> <a href="https://3dcomputervision.github.io/about/">Avinash Sharma</a><sup>1</sup> </span> <br /> <span class="author-block"><sup>1</sup>International Institute of Information Technology Hyderabad, India</span></div><div align="center"> <span class="link-block"> <a href="/assets/pdf/papers/diff_param_main.pdf" class="external-link button is-small is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span> Main Paper</span> </a> </span> <span class="link-block"> <a href="/assets/pdf/papers/diff_param_suppl.pdf" class="external-link button is-small is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Supplementary</span> </a> </span> <!-- <span class="link-block"> --> <!-- <a href="https://youtu.be/Nc2cdQQ__-Y" class="external-link button is-small is-rounded is-dark"> <span class="icon"> <i class="fab fa-youtube"></i> </span> <span>Presentation</span> </a> --> <!-- </span> --></div><div style="text-align:center"> <i>(Follow more at <a href="https://3dcomputervision.github.io">3DVisLab</a>​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​)</i></div><p>​</p><p><br /> <img src="/assets/project_pages/diff_param/teaser_diff_param.png" style="width:100% ; height:auto" /> <br /><br /></p><h3>Abstract</h3><p> We present a novel self-supervised framework for learning the discretization-agnostic surface parameterization of arbitrary 3D objects with both open and closed surfaces. Our framework leverages diffusion-enabled global-to-local shape context for each vertex first to partition the closed surface into multiple patches using the proposed self-supervised PatchNet and subsequently perform independent UV parameterization of these patches by learning forward and backward UV mapping for individual patches. Thus, our framework enables learning a discretization-agnostic parameterization at a lower resolution and then directly inferring the parameterization for a higher-resolution mesh without retraining. We evaluate our framework on multiple 3D objects from the publicly available SHREC dataset and report superior/faster UV parameterization over conventional methods.</p><!-- <br> --><p><!--<div align="center"> --> <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/Nc2cdQQ__-Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> --> <!--</div>--></p><h3>Method</h3><h4>Patch Extraction</h4><p><img src="/assets/project_pages/diff_param/patch_extraction.png" style="width:100% ; height:auto" /> <br /></p><h4>Surface Parameterization</h4><p><img src="/assets/project_pages/diff_param/param.png" style="width:100% ; height:auto" /> <br /><br /></p><hr /><h2>Parameterization in Action</h2><p><br /></p><h4>Open Surfaces (Hemisphere)</h4><p align="center"> <img src="/assets/project_pages/diff_param/hemisphere.gif" style="width:90% ; height:auto" /></p><hr /><h4>Open Surfaces (Face)</h4><p align="center"> <img src="/assets/project_pages/diff_param/face.gif" style="width:90% ; height:auto" /></p><hr /><h4>Closed Surfaces (Bob)</h4><p align="center"> <img src="/assets/project_pages/diff_param/duck.gif" style="width:90% ; height:auto" /></p><hr /><h4>Closed Surfaces (Sphere)</h4><p align="center"> <img src="/assets/project_pages/diff_param/sphere.gif" style="width:90% ; height:auto" /></p><hr /><h3>Related Works</h3><ul><li><a href="https://threedle.github.io/DA-Wand/">DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization</a><li><a href="https://geometrycollective.github.io/boundary-first-flattening/">Boundary First Flattening</a><li><a href="http://www.cs.ubc.ca/labs/imager/tr/2018/OptCuts/">OptCuts: Joint Optimization of Surface Cuts and Parameterization</a></ul><hr /><h3>Acknowledgements</h3><ul><li>We thank the reviewers of our SIGGRAPH Asia 2022 submission for their valuable comments and suggestions.<li>Keenan Crane for open-sourcing the <a href="https://www.cs.cmu.edu/~kmcrane/Projects/ModelRepository/">Spot</a> and <a href="https://www.cs.cmu.edu/~kmcrane/Projects/ModelRepository/">Bob</a> mesh.<li>Dhawal Sirikonda for helping us with the visualisation of the QCE error metric.</ul><hr /><h3>BibTex</h3><div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3550340.3564235</span><span class="p">,</span>
<span class="na">author</span> <span class="p">=</span> <span class="s">{Pokhariya, Chandradeep and Naik, Shanthika and Srivastava, Astitva and Sharma, Avinash}</span><span class="p">,</span>
<span class="na">title</span> <span class="p">=</span> <span class="s">{Discretization-Agnostic Deep Self-Supervised 3D Surface Parameterization}</span><span class="p">,</span>
<span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450394659}</span><span class="p">,</span>
<span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
<span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
<span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3550340.3564235}</span><span class="p">,</span>
<span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3550340.3564235}</span><span class="p">,</span>
<span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SIGGRAPH Asia 2022 Technical Communications}</span><span class="p">,</span>
<span class="na">articleno</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="na">numpages</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="na">keywords</span> <span class="p">=</span> <span class="s">{UV parameterization, self-supervised learning, texture mapping, neural network, surface parameterization.}</span><span class="p">,</span>
<span class="na">location</span> <span class="p">=</span> <span class="s">{Daegu, Republic of Korea}</span><span class="p">,</span>
<span class="na">series</span> <span class="p">=</span> <span class="s">{SA '22}</span>
<span class="p">}</span>
</code></pre></div></div></main>
